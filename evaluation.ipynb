{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmeadows17/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-08 22:45:41.985959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-08 22:45:41.997627: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-08 22:45:42.001303: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-08 22:45:42.010160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-08 22:45:42.740069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3080. Max memory: 10.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:17<00:00,  4.44s/it]\n",
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n",
      "/home/jmeadows17/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, sp.Integer):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# Define a function to apply templates to conversations\n",
    "def apply_template(examples):\n",
    "    messages = examples[\"conversations\"]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Define a function for prompting the model\n",
    "def prompt_model(prompt, model, tokenizer):\n",
    "\n",
    "    messages = [\n",
    "        {\"from\":\"human\", \"value\":prompt}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    generation = model.generate(input_ids=inputs, max_new_tokens=400, use_cache=True)\n",
    "    decoded_seq = tokenizer.decode(generation[0],\n",
    "                                   skip_special_tokens=True,\n",
    "                                   do_sample=False)\n",
    "    return decoded_seq.split(\"assistant\")[1].replace(prompt, \"\").replace(\"Derivation:\",\"\")\n",
    "\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "#model_name = \"MathLLaMa-3.1-8B\"\n",
    "\n",
    "# Set parameters\n",
    "max_seq_length = 1024\n",
    "batch_size = 1\n",
    "learning_rate = 5e-5\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# Configure the tokenizer with chat templates\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "    chat_template=\"chatml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(model, \"best_loras/epoch_6\")\n",
    "# model.push_to_hub_merged(\"jmeadows17/MathLLaMa-3.1-8B\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(epoch, eval, few_shot, model, tokenizer, data, lora=False):\n",
    "    # saves {prompt, prediction, reference, rouge, bleu, gleu} per example in data as a list (json)\n",
    "    # outputs average {rouge, bleu, gleu} as a dict\n",
    "\n",
    "    all_results = []\n",
    "    metrics = [\"rouge\",\"bleu\",\"gleu\"]\n",
    "\n",
    "    #zero_shot_preamble = 'Derive the final equation using the premise equations from the following prompt (denoted by \"Prompt:\"). Give only the equations involved in the derivation. Do not include any text other than equations each separated by \"and\". Prompt: '\n",
    "\n",
    "    # keys for dataset specific values\n",
    "    if eval != \"static\":\n",
    "        prompt_key = f\"{eval} prompt\"\n",
    "        derivation_key = f\"{eval} derivation\"\n",
    "    else:\n",
    "        prompt_key = \"prompt\"\n",
    "        derivation_key = \"derivation\"\n",
    "\n",
    "    # load LoRA and set up model for inference\n",
    "    if lora == True:\n",
    "        path = f\"best_loras/epoch_{epoch}\"\n",
    "        lora_model = PeftModel.from_pretrained(model, path)\n",
    "    else:\n",
    "        lora_model = model \n",
    "    with torch.no_grad():\n",
    "        FastLanguageModel.for_inference(lora_model)\n",
    "        tokenizer.padding_side = \"left\"\n",
    "\n",
    "        # begin inference and evaluation per example\n",
    "        for example in tqdm(data):\n",
    "            if few_shot is True:\n",
    "                prompt = example[prompt_key]\n",
    "            else:\n",
    "                prompt = example[prompt_key].split(\"Prompt: \")[-1]\n",
    "            reference = example[derivation_key]\n",
    "            prediction = prompt_model(prompt, lora_model, tokenizer)\n",
    "            \n",
    "            # initialise results dictionary per example\n",
    "            results = {\n",
    "                \"few-shot \" + prompt_key if few_shot is True else prompt_key : prompt,\n",
    "                derivation_key + \" prediction\" : prediction,\n",
    "                derivation_key + \" reference\" : reference,\n",
    "            }\n",
    "\n",
    "            # calculate scores\n",
    "            for metric_name in metrics:\n",
    "\n",
    "                if metric_name == \"gleu\":\n",
    "                    metric = load(\"google_bleu\")\n",
    "                elif metric_name == \"bleurt\":\n",
    "                    metric = load(\"bleurt\", \"bleurt-large-512\")\n",
    "                else:\n",
    "                    metric = load(metric_name)\n",
    "                try:\n",
    "                    m = metric.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "                    if metric_name == \"rouge\":\n",
    "                        score = m[\"rouge2\"]\n",
    "                    elif metric_name == \"bleu\":\n",
    "                        score = m[\"bleu\"]\n",
    "                    elif metric_name == \"gleu\":\n",
    "                        score = m[\"google_bleu\"]\n",
    "                    elif metric_name == \"bleurt\":\n",
    "                        score = m[\"scores\"][0]\n",
    "                except:\n",
    "                    score = 0.0\n",
    "\n",
    "                results[metric_name] = score\n",
    "            all_results.append(results)\n",
    "\n",
    "            # save current results to json after appending\n",
    "            with open(f\"{eval}_epoch={epoch}_few-shot={few_shot}.json\",\"w\") as f:\n",
    "                json.dump(all_results, f, cls=NpEncoder)\n",
    "    \n",
    "    # calculate average scores\n",
    "    averages = {}\n",
    "    for metric_name in metrics:\n",
    "        averages[metric_name] = round(np.mean([i[metric_name] for i in all_results])*100, 1)\n",
    "\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:38<00:00, 19.39s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge': 41.5, 'bleu': 40.1, 'gleu': 40.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"gpt-4_results.json\") as f:\n",
    "    data = json.load(f)\n",
    "data = data[:2] #for testing code works\n",
    "\n",
    "eval_modes = [\"static\"]\n",
    "few_shot_modes = [False]\n",
    "\n",
    "for eval in tqdm(eval_modes):\n",
    "    for few_shot in few_shot_modes:\n",
    "\n",
    "        # evaluation\n",
    "        scores = evaluate_model(\n",
    "            epoch=1000,\n",
    "            eval=eval, \n",
    "            few_shot=few_shot, \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            data=data,\n",
    "            lora=False,\n",
    "        )\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [23:32<00:00, 14.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = static few-shot = False\n",
      "{'epoch': 1, 'rouge': 89.4, 'bleu': 84.1, 'gleu': 85.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [24:14<00:00, 14.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = static few-shot = False\n",
      "{'epoch': 6, 'rouge': 91.4, 'bleu': 87.5, 'gleu': 88.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [28:01<00:00, 16.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = static few-shot = True\n",
      "{'epoch': 1, 'rouge': 81.8, 'bleu': 76.1, 'gleu': 77.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [28:31<00:00, 17.12s/it]\n",
      " 20%|â–ˆâ–ˆ        | 1/5 [1:44:24<6:57:38, 6264.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = static few-shot = True\n",
      "{'epoch': 6, 'rouge': 85.7, 'bleu': 79.2, 'gleu': 80.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [21:44<00:00, 13.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = VR few-shot = False\n",
      "{'epoch': 1, 'rouge': 89.4, 'bleu': 85.4, 'gleu': 86.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [21:56<00:00, 13.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = VR few-shot = False\n",
      "{'epoch': 6, 'rouge': 91.9, 'bleu': 88.0, 'gleu': 88.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [24:25<00:00, 14.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = VR few-shot = True\n",
      "{'epoch': 1, 'rouge': 83.5, 'bleu': 78.9, 'gleu': 79.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [26:13<00:00, 15.73s/it]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [3:18:47<4:55:31, 5910.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = VR few-shot = True\n",
      "{'epoch': 6, 'rouge': 83.8, 'bleu': 78.4, 'gleu': 79.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [25:45<00:00, 15.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = EE few-shot = False\n",
      "{'epoch': 1, 'rouge': 85.1, 'bleu': 78.9, 'gleu': 79.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [26:00<00:00, 15.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = EE few-shot = False\n",
      "{'epoch': 6, 'rouge': 85.1, 'bleu': 78.6, 'gleu': 79.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [27:10<00:00, 16.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = EE few-shot = True\n",
      "{'epoch': 1, 'rouge': 82.2, 'bleu': 75.3, 'gleu': 76.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [30:27<00:00, 18.27s/it]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [5:08:13<3:26:59, 6209.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = EE few-shot = True\n",
      "{'epoch': 6, 'rouge': 80.7, 'bleu': 73.6, 'gleu': 74.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [23:58<00:00, 14.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = AG few-shot = False\n",
      "{'epoch': 1, 'rouge': 88.1, 'bleu': 81.8, 'gleu': 83.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [24:04<00:00, 14.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = AG few-shot = False\n",
      "{'epoch': 6, 'rouge': 91.3, 'bleu': 87.0, 'gleu': 87.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [27:55<00:00, 16.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = AG few-shot = True\n",
      "{'epoch': 1, 'rouge': 79.4, 'bleu': 72.2, 'gleu': 74.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [28:51<00:00, 17.31s/it]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [6:53:05<1:44:02, 6242.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = AG few-shot = True\n",
      "{'epoch': 6, 'rouge': 84.0, 'bleu': 76.7, 'gleu': 77.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [22:47<00:00, 13.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = SR few-shot = False\n",
      "{'epoch': 1, 'rouge': 79.4, 'bleu': 70.5, 'gleu': 73.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [24:09<00:00, 14.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = SR few-shot = False\n",
      "{'epoch': 6, 'rouge': 81.0, 'bleu': 72.6, 'gleu': 74.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [25:42<00:00, 15.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = SR few-shot = True\n",
      "{'epoch': 1, 'rouge': 75.9, 'bleu': 67.8, 'gleu': 70.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [27:20<00:00, 16.41s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [8:33:08<00:00, 6157.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval = SR few-shot = True\n",
      "{'epoch': 6, 'rouge': 76.4, 'bleu': 68.5, 'gleu': 70.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model results for picking lora\n",
    "\n",
    "# data\n",
    "with open(\"gpt-4_results.json\") as f:\n",
    "    data = json.load(f)\n",
    "#data = data[:2] #for testing code works\n",
    "\n",
    "eval_modes = [\"static\",\"VR\",\"EE\",\"AG\",\"SR\"]\n",
    "few_shot_modes = [False, True]\n",
    "results = []\n",
    "\n",
    "for eval in tqdm(eval_modes):\n",
    "    for few_shot in few_shot_modes:\n",
    "        for epoch in [1,6]:\n",
    "\n",
    "            # evaluation\n",
    "            scores = evaluate_model(\n",
    "                epoch=epoch,\n",
    "                eval=eval, \n",
    "                few_shot=few_shot, \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                data=data,\n",
    "                lora=True,\n",
    "                save=False,\n",
    "            )\n",
    "            print(f\"eval = {eval}\", f\"few-shot = {few_shot}\")\n",
    "            dict = {\"epoch\":epoch} | scores\n",
    "            results.append(dict)\n",
    "            print(dict)\n",
    "            with open(\"lora_results.json\",\"w\") as f:\n",
    "                json.dump(results,f,cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging LoRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [23:46<00:00, 14.26s/it]\n",
      " 20%|â–ˆâ–ˆ        | 1/5 [23:47<1:35:08, 1427.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'rouge': 91.9, 'bleu': 88.2, 'gleu': 88.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [23:51<00:00, 14.31s/it]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [47:39<1:11:29, 1429.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'rouge': 91.0, 'bleu': 86.8, 'gleu': 87.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [23:27<00:00, 14.07s/it]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [1:11:06<47:19, 1419.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'rouge': 91.9, 'bleu': 87.8, 'gleu': 88.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [23:30<00:00, 14.11s/it]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [1:34:38<23:36, 1416.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'rouge': 90.5, 'bleu': 85.8, 'gleu': 86.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [22:57<00:00, 13.77s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [1:57:36<00:00, 1411.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'rouge': 91.6, 'bleu': 87.4, 'gleu': 88.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model results for picking lora\n",
    "\n",
    "# data\n",
    "with open(\"gpt-4_results.json\") as f:\n",
    "    data = json.load(f)\n",
    "data = data #for testing code works\n",
    "\n",
    "eval = \"static\"\n",
    "few_shot = False\n",
    "results = []\n",
    "for epoch in tqdm(range(6, 11)):\n",
    "\n",
    "    # evaluation\n",
    "    scores = evaluate_model(\n",
    "        epoch=epoch,\n",
    "        eval=eval, \n",
    "        few_shot=few_shot, \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        data=data\n",
    "    )\n",
    "\n",
    "    dict = {\"epoch\":epoch} | scores\n",
    "    results.append(dict)\n",
    "    print(dict)\n",
    "    with open(\"lora_results.json\",\"w\") as f:\n",
    "        json.dump(results,f,cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
